# -*- coding: utf-8 -*-
"""Copy of lab03.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A6Z1GgfqUizcDEFd2geQYyau8T8OEkkZ
"""

# Initialize Otter
!pip install otter-grader
import otter
grader = otter.Notebook("lab03.ipynb")

"""# Lab 3: Data Cleaning and Visualization

In this lab you will be working on visualizing a dataset from the City of Berkeley containing data on calls to the Berkeley Police Department. Information about the dataset can be found [at this link](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Calls-for-Service/k2nh-s5h5).

Note: This lab will not work on older versions of Python; make sure to work on DataHub.

**This assignment should be completed and submitted by 11:59 PM PDT on Tuesday, September 14th, 2021.**

**Content Warning: This lab includes an analysis of crime in Berkeley. If you feel uncomfortable about the topic, please feel free to contact your GSI or the instructors.**

## Setup

Note that we configure a custom default figure size. Virtually every default aspect of matplotlib [can be customized](https://matplotlib.org/users/customizing.html).
"""

import pandas as pd
import numpy as np
import zipfile
import matplotlib
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (12, 9)

"""## Part 1: Cleaning and Exploring the Data

To retrieve the dataset, we will use the `ds100_utils.fetch_and_cache` utility.
"""

#import ds100_utils

#data_dir = 'data'
#data_url = 'http://www.ds100.org/fa20/resources/assets/datasets/lab04_data_fa20.zip'
#file_name = 'lab04_data_fa20.zip'

#dest_path = ds100_utils.fetch_and_cache(data_url=data_url, file=file_name, data_dir=data_dir)
#print(f'Located at {dest_path}')
import requests
from pathlib import Path

def fetch_and_cache(data_url, file, data_dir="data", force=False):
    """
    Download and cache a url and return the file object.
    
    data_url: the web address to download
    file: the file in which to save the results.
    data_dir: (default="data") the location to save the data
    force: if true the file is always re-downloaded 
    
    return: The pathlib.Path to the file.
    """
    data_dir = Path(data_dir)
    data_dir.mkdir(exist_ok=True)
    file_path = data_dir/Path(file)
    if force and file_path.exists():
        file_path.unlink()
    if force or not file_path.exists():
        print('Downloading...', end=' ')
        resp = requests.get(data_url)
        with file_path.open('wb') as f:
            f.write(resp.content)
        print('Done!')
    else:
        import time 
        created = time.ctime(file_path.stat().st_ctime)
        print("Using cached version downloaded at", created)
    return file_path

data_url = 'http://www.ds100.org/fa20/resources/assets/datasets/lab04_data_fa20.zip'
dest_path = fetch_and_cache(data_url, 'lab05_data_fa20.zip')

"""We will now directly unzip the ZIP archive and start working with the uncompressed files.

Note: There is no single right answer regarding whether to work with compressed files in their compressed state or to uncompress them on disk permanently. If you for example need to work with multiple tools on the same files, or write many notebooks to analyze them, and they are not too large, it may be more convenient to uncompress them once.  But you may also have situations where you find it preferable to work with the compressed data directly.  

Python gives you tools for both approaches, and you should know how to perform both tasks in order to choose the one that best suits the problem at hand.

---

Run the cell below to extract the zip file into the data directory.
"""

my_zip = zipfile.ZipFile(dest_path, 'r')
my_zip.extractall("data")

"""Now, we'll use a method of the `Pathlib.Path` class called `glob` to list all files in the `data` directory. You will find useful information in pathlib [documentation](https://docs.python.org/3/library/pathlib.html).

Below, we use pathlib's `glob` method to store the list of all files' names from the `data_dir` directory in the variable `file_names`. These names should be strings that contain only the file name (e.g. `dummy.txt` not `data/dummy.txt`). The asterisk (*) character is used with the `glob` method to match any string.
"""

from pathlib import Path
data_dir_path = Path('data') # creates a Path object that points to the data directory
file_names = [x.name for x in data_dir_path.glob('*') if x.is_file()]
file_names

"""Let's now load the CSV file we have into a `pandas.DataFrame` object."""

calls = pd.read_csv("data/Berkeley_PD_-_Calls_for_Service.csv")
calls

"""We see that the fields include a case number, the offense type, the date and time of the offense, the "CVLEGEND" which appears to be related to the offense type, a "CVDOW" which has no apparent meaning, a date added to the database, and the location spread across four fields.

Let's also check some basic information about these files using the `DataFrame.info` and `DataFrame.describe` methods.
"""

calls.info()

"""Note that the `BLKADDR` column only has 3766 non-null entries, while the other columns all have 3788 entries. This is because the `.info()` method only counts non-null entries."""

calls.describe()

"""Notice that the functions above reveal type information for the columns, as well as some basic statistics about the numerical columns found in the DataFrame. However, we still need more information about what each column represents. Let's explore the data further in Question 1.

Before we go over the fields to see their meanings, the cell below will verify that all the events happened in Berkeley by grouping on the `City` and `State` columns. You should see that all of our data falls into one group.
"""

calls.groupby(["City","State"]).count()

"""When we called `head()` on the Dataframe `calls`, it seemed like `OFFENSE` and `CVLEGEND` both contained information about the type of event reported. What is the difference in meaning between the two columns? One way to probe this is to look at the `value_counts` for each Series."""

calls['OFFENSE'].value_counts().head(10)

calls['CVLEGEND'].value_counts().head(10)

"""It seems like `OFFENSE` is more specific than `CVLEGEND`, e.g. "LARCENY" vs. "THEFT FELONY (OVER $950)". If you're unfamiliar with the term, "larceny" is a legal term for theft of personal property.

To get a sense of how many subcategories there are for each `OFFENSE`, we will set `calls_by_cvlegend_and_offense` equal to a multi-indexed series where the data is first indexed on the `CVLEGEND` and then on the `OFFENSE`, and the data is equal to the number of offenses in the database that match the respective `CVLEGEND` and `OFFENSE`. As you can see, `calls_by_cvlegend_and_offense["LARCENY", "THEFT FROM PERSON"]` returns 24 which means there are 24 instances of larceny with offense of type "THEFT FROM PERSON" in the database.
"""

calls_by_cvlegend_and_offense = calls.groupby(["CVLEGEND", "OFFENSE"]).size()
calls_by_cvlegend_and_offense["LARCENY", "THEFT FROM PERSON"]

"""### Question 1

In the cell below, set `answer1` equal to a list of strings corresponding to the possible values for `OFFENSE` when `CVLEGEND` is "LARCENY". You can type the answer manually, or you can create an expression that automatically extracts the names.

<!--
BEGIN QUESTION
name: q1
-->
"""

#answer1 = ...
larcenyDF = calls[calls['CVLEGEND'].str.fullmatch('LARCENY')]
larcenyOffenseList = larcenyDF['OFFENSE'].values.tolist()
answer1 = []
[answer1.append(x) for x in larcenyOffenseList if x not in answer1]
print(answer1)

grader.check("q1")

"""--- 

## Part 2: Visualization

## Pandas Examples

Pandas offers basic functionality for plotting. For example, the `DataFrame` and `Series` classes both have a `plot` method. 

As you learn to do data visualization, you may find the [pandas plotting documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)  helpful!

As an example of the built-in plotting functionality of pandas, the following example uses `plot` method of the `Series` class to generate a `barh` plot type to visually display the value counts for `CVLEGEND`.

There are also many other plots that we will explore throughout the lab.
"""

ax = calls['CVLEGEND'].value_counts().plot(kind='barh')
ax.set_ylabel("Crime Category")
ax.set_xlabel("Number of Calls")
ax.set_title("Number of Calls By Crime Type");
ax2 = plt.gca()

"""## An Additional Note on Plotting in Jupyter Notebooks

You may have noticed that many of our code cells involving plotting end with a semicolon (;). This prevents any extra output from the last line of the cell that we may not want to see. Try adding this to your own code in the following questions!

### Question 2

Now it is your turn to make some plots using `pandas`.  Let's start by transforming the data so that it is easier to work with. We then will look at some distributions of the data. 

The CVDOW field isn't named helpfully and it is hard to see the meaning from the data alone. According to the website linked at the top of this notebook, CVDOW is actually indicating the day that events happened. 0->Sunday, 1->Monday ... 6->Saturday. 

#### Question 2a

Add a new column `Day` into the `calls` DataFrame that has the string weekday (eg. 'Sunday') for the corresponding value in CVDOW. For example, if the first 3 values of `CVDOW` are `[3, 6, 0]`, then the first 3 values of the `Day` column should be `["Wednesday", "Saturday", "Sunday"]`.

**Hint:** *Try using the [Series.map](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html) function on `calls["CVDOW"]`.  Can you assign this to the new column `calls["Day"]`?*

<!--
BEGIN QUESTION
name: q2a
-->
"""

days = ["Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"]
day_indices = range(7)
indices_to_days_dict = dict(zip(day_indices, days)) # Should look like {0:"Sunday", 1:"Monday", ..., 6:"Saturday"}
calls["Day"]=calls["CVDOW"].map(indices_to_days_dict)
calls["Day"].value_counts()
ax = calls["Day"].value_counts().reindex(days).plot(kind='bar')

grader.check("q2a")

"""#### Question 2b

Now let's look at the EVENTTM column which indicates the time for events. Since it contains hour and minute information, let's extract the hour info and create a new column named `Hour` in the `calls` DataFrame. You should save the hour as an `int`.


**Hint:** *Your code should only require one line*

<!--
BEGIN QUESTION
name: q2b
-->
"""

hours = list(range(24))
calls["Hour"] = calls['EVENTTM'].str[:2].map(int)
ax = calls["Hour"].value_counts().reindex(hours).plot(kind='bar')

grader.check("q2b")

"""#### Question 2c


Using `pandas`, construct a line plot with the count of the number of calls (entries in the table) for each hour of the day  **ordered by the time** (eg. `12:00 AM`, `1:00 AM`, ...). Please use the provided variable `hours` in your answer. Be sure that your axes are labeled and that your plot is titled. 

<!--
BEGIN QUESTION
name: q2c

-->
"""

calls_robbery = calls[calls['CVLEGEND']=="ROBBERY"]
hour = list(range(24))
calls["Hour"] = calls_robbery['EVENTTM'].str[:2].map(int)
ax = calls["Hour"].value_counts().reindex(hour).plot(kind='line')


# Leave this for grading purposes
ax_3d = plt.gca()

grader.check("q2c")

"""##### Why do you think this happens? Are there more calls in the day or night? What are the most and least popular times? (Answer Below)

I would say that most robberies happen around 11pm

### Question 3

We can break down into some particular types of events to see their distribution. For example, let's make a bar plot for the CVLEGEND "ROBBERY". 

#### Question 3a

Use `pandas` to create a vertical bar plot of the number of total robberies reported on each day of the week, again ordered by the days of the week starting with Sunday. Please use the provided variable `filtered` in your answer. Be sure that your axes are labeled and that your plot is titled.

**Hint:** *This should be very similar to Question 2c*

<!--
BEGIN QUESTION
name: q3a
-->
"""

filtered = calls[calls["CVLEGEND"] == "ROBBERY"]
calls['DATE'] = pd.to_datetime(calls['EVENTDT'])
calls['Day']= calls['DATE'].dt.day_name()
dayList=['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
ax = filtered['Day'].value_counts().reindex(dayList).plot(kind='bar')

# Leave this for grading purposes
ax_4a = plt.gca()

grader.check("q3a")

"""#### Question 3b

Do you observe anything interesting about the distribution of ROBBERY calls over a week? Which day is the peak for "ROBBERY"? Type a 1-2 sentence answer below.

The historically most common day that robberies occur in this area is thursday

### Question 4

In the cell below, generate a boxplot which examines the hour of day of each crime broken down by the `CVLEGEND` value.  To construct this plot use the [DataFrame.boxplot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html) documentation. You may want to rotate the `CVLEGEND` labels for better readability.

##### Looking at your plot, which crime type appears to have the largest interquartile range? Put your results into `answer4` as a string.


<!--
BEGIN QUESTION
name: q4
-->
"""

ax = calls.boxplot(column=["Hour"], by="CVLEGEND", rot=90)

grader.check("q4")

"""**Important**: To make sure the test cases run correctly, click `Kernel>Restart & Run All` and make sure all of the test cases are still passing. Doing so will submit your code for you. 

If your test cases are no longer passing after restarting, it's likely because you're missing a variable, or the modifications that you'd previously made to your DataFrame are no longer taking place (perhaps because you deleted a cell). 

You may submit this assignment as many times as you'd like before the deadline.

**You must restart and run all cells before submitting. Otherwise, you may pass test cases locally, but not on our servers. We will not entertain regrade requests of the form, “my code passed all of my local test cases, but failed the autograder”.**

## Congratulations

Congrats! You are finished with this assignment.

---

To double-check your work, the cell below will rerun all of the autograder tests.
"""

grader.check_all()

"""## Submission

Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**
"""

# Save your notebook first, then run this cell to export your submission.
grader.export(pdf=False)